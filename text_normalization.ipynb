{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNYs9lSNyMRj6AvvquAxrzO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Akshaay23/NLP_Learning/blob/main/text_normalization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Text normalization is the process of transforming text into a standard, consistent form to improve the quality of data used in NLP tasks. It is a crucial preprocessing step in NLP and includes several techniques:\n",
        "\n",
        "### **Key Text Normalization Techniques:**\n",
        "\n",
        "1. **Lowercasing**  \n",
        "   - Converts all text to lowercase to maintain consistency.  \n",
        "   - Example: `\"Hello WORLD!\" → \"hello world!\"`\n",
        "\n",
        "2. **Removing Punctuation**  \n",
        "   - Eliminates punctuation marks that do not contribute to meaning in many NLP tasks.  \n",
        "   - Example: `\"Hello, world!\" → \"Hello world\"`\n",
        "\n",
        "3. **Removing Stopwords**  \n",
        "   - Removes common words (e.g., \"is\", \"the\", \"and\") that do not add much meaning.  \n",
        "   - Example: `\"This is a good day\" → \"good day\"`\n",
        "\n",
        "4. **Tokenization**  \n",
        "   - Splits text into individual words or subwords.  \n",
        "   - Example: `\"I love NLP\"` → `[\"I\", \"love\", \"NLP\"]`\n",
        "\n",
        "5. **Lemmatization**  \n",
        "   - Converts words to their base or dictionary form.  \n",
        "   - Example: `\"running\" → \"run\"`, `\"better\" → \"good\"`\n",
        "\n",
        "6. **Stemming**  \n",
        "   - Reduces words to their root form by chopping off suffixes.  \n",
        "   - Example: `\"running\" → \"run\"`, `\"flies\" → \"fli\"`\n",
        "\n",
        "7. **Expanding Contractions**  \n",
        "   - Converts contractions into their full form.  \n",
        "   - Example: `\"don't\"` → `\"do not\"`, `\"you're\"` → `\"you are\"`\n",
        "\n",
        "8. **Removing Special Characters and Numbers**  \n",
        "   - Eliminates characters like `@`, `#`, `!`, and numbers unless necessary.  \n",
        "   - Example: `\"Hello @world123!\"` → `\"Hello world\"`\n",
        "\n",
        "9. **Spelling Correction**  \n",
        "   - Corrects common spelling errors using libraries like `textblob` or `hunspell`.  \n",
        "   - Example: `\"teh\" → \"the\"`, `\"recieve\" → \"receive\"`\n",
        "\n",
        "10. **Handling Accents and Unicode Characters**  \n",
        "   - Converts accented characters into their simple forms.  \n",
        "   - Example: `\"café\" → \"cafe\"`, `\"résumé\" → \"resume\"`\n",
        "\n",
        "11. **Dealing with Emojis and Emoticons**  \n",
        "   - Can either remove them or convert them into textual meanings.  \n",
        "#   - Example: `\"😊\"` → `\"smiling face\"`\n"
      ],
      "metadata": {
        "id": "0nEq9VdYmIKu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Download necessary NLTK resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "def normalize_text(text):\n",
        "    \"\"\"\n",
        "    Function to normalize text by:\n",
        "    - Lowercasing\n",
        "    - Removing special characters and punctuation\n",
        "    - Tokenization\n",
        "    - Removing stopwords\n",
        "    - Lemmatization\n",
        "    \"\"\"\n",
        "    # Convert text to lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # Remove special characters, numbers, and punctuation\n",
        "    text = re.sub(r\"[^a-z\\s]\", \"\", text)\n",
        "\n",
        "    # Tokenization\n",
        "    words = word_tokenize(text)\n",
        "\n",
        "    # Remove stopwords\n",
        "    stop_words = set(stopwords.words(\"english\"))\n",
        "    words = [word for word in words if word not in stop_words]\n",
        "\n",
        "    # Lemmatization\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    words = [lemmatizer.lemmatize(word) for word in words]\n",
        "\n",
        "    # Join words back into a normalized sentence\n",
        "    return \" \".join(words)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uBkFl230nIKs",
        "outputId": "96e23dea-fcfc-4bc2-ddd6-4acde21fd078"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "example running jumping coding fun\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "text = \"\"\"Hey there! 😊 John, a software engineer at Google, is running late for his meeting on Monday at 10:30 AM. He’s been working on an AI project that's gonna revolutionize the industry! 🚀 But isn’t it crazy how fast technology evolves? Just last year (in 2023), they released an AI model that outperforms humans in chess!\n",
        "\n",
        "Meanwhile, in New York, Emma ordered a coffee ☕ for $5.99 and thought, 'This better be good!' She checked her phone 📱— 100+ unread emails! 😡 Btw, have you seen @ElonMusk's latest tweet? #AI #FutureOfTech.\"\"\"\n",
        "normalized_text = normalize_text(text)\n",
        "print(normalized_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r9zyrpi9oeoX",
        "outputId": "c792505b-0b69-419d-c746-6906306d12a7"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hey john software engineer google running late meeting monday he working ai project thats gon na revolutionize industry isnt crazy fast technology evolves last year released ai model outperforms human chess meanwhile new york emma ordered coffee thought better good checked phone unread email btw seen elonmusks latest tweet ai futureoftech\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "using spacy\n",
        "\n",
        "Why Use spaCy?\n",
        "\n",
        "✅ Faster & optimized (written in Cython)\n",
        "\n",
        "✅ Pre-trained NLP models\n",
        "\n",
        "✅ Better lemmatization than NLTK"
      ],
      "metadata": {
        "id": "umm2LSP4o8bU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "import re\n",
        "\n",
        "# Load spaCy English model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Emoji removal regex\n",
        "EMOJI_PATTERN = re.compile(\"[\\U0001F600-\\U0001F64F\\U0001F300-\\U0001F5FF\\U0001F680-\\U0001F6FF\\U0001F700-\\U0001F77F\"\n",
        "                           \"\\U0001F780-\\U0001F7FF\\U0001F800-\\U0001F8FF\\U0001F900-\\U0001F9FF\\U0001FA00-\\U0001FA6F\"\n",
        "                           \"\\U0001FA70-\\U0001FAFF]+\", flags=re.UNICODE)\n",
        "\n",
        "def normalize_text_spacy(text):\n",
        "    \"\"\"\n",
        "    Function to normalize text using spaCy:\n",
        "    - Lowercasing\n",
        "    - Removing punctuation, special characters, and emojis\n",
        "    - Tokenization\n",
        "    - Removing stopwords\n",
        "    - Lemmatization\n",
        "    \"\"\"\n",
        "    # Remove emojis\n",
        "    text = EMOJI_PATTERN.sub(\"\", text)\n",
        "\n",
        "    # Convert to lowercase and process with spaCy\n",
        "    doc = nlp(text.lower())\n",
        "\n",
        "    words = [\n",
        "        token.lemma_ for token in doc\n",
        "        if not token.is_stop and not token.is_punct and not token.is_space\n",
        "    ]\n",
        "\n",
        "    return \" \".join(words)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jUjofnKEooNY",
        "outputId": "05ac19e5-5b11-49c3-eba7-6973447f9a19"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "example running jumping code fun\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "text = \"\"\"Hey there! 😊 John, a software engineer at Google, is running late for his meeting on Monday at 10:30 AM. He’s been working on an AI project that's gonna revolutionize the industry! 🚀 But isn’t it crazy how fast technology evolves? Just last year (in 2023), they released an AI model that outperforms humans in chess!\n",
        "\n",
        "Meanwhile, in New York, Emma ordered a coffee ☕ for $5.99 and thought, 'This better be good!' She checked her phone 📱— 100+ unread emails! 😡 Btw, have you seen @ElonMusk's latest tweet? #AI #FutureOfTech.\"\"\"\n",
        "normalized_text = normalize_text_spacy(text)\n",
        "print(normalized_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N0xXvYkGpAk1",
        "outputId": "24f765c6-0709-46e1-9a44-91e680bb009c"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hey john software engineer google run late meeting monday 10:30 work ai project go to revolutionize industry crazy fast technology evolve year 2023 release ai model outperform human chess new york emma order coffee ☕ $ 5.99 think well good check phone 100 + unread email btw see @elonmusk late tweet ai futureoftech\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "H0UzMgGRpCH0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Both **spaCy** and **NLTK** are widely used NLP libraries, but they serve different purposes. Choosing between them depends on your project requirements.  \n",
        "\n",
        "---\n",
        "\n",
        "## ** When to Use NLTK?**\n",
        " **Best for research, experimentation, and academic use**  \n",
        " **Highly customizable for rule-based NLP**  \n",
        " **Good for small-scale projects or text preprocessing**  \n",
        " **Provides in-depth linguistic features (e.g., grammar parsing, WordNet)**  \n",
        " **Useful for statistical NLP, POS tagging, and chunking**  \n",
        "\n",
        "🔹 **Use NLTK if:**  \n",
        "- You need full control over NLP preprocessing.  \n",
        "- You’re working on **linguistics-based** projects or research.  \n",
        "- You need **custom tokenization, stemming, and POS tagging**.  \n",
        "- You’re working with **low-resource languages** (spaCy supports only a few).  \n",
        "\n",
        "---\n",
        "\n",
        "## ** When to Use spaCy?**\n",
        " **Best for production-level NLP applications**  \n",
        " **Faster and optimized for performance (Cython-based)**  \n",
        " **Pre-trained NLP models for Named Entity Recognition (NER), POS tagging, and Dependency Parsing**  \n",
        " **More efficient and accurate lemmatization**  \n",
        " **Handles large-scale text processing**  \n",
        "\n",
        "🔹 **Use spaCy if:**  \n",
        "- You need a **fast, scalable, and production-ready** NLP pipeline.  \n",
        "- You’re building a **chatbot, search engine, or text classification model**.  \n",
        "- You need **Named Entity Recognition (NER), dependency parsing, or document similarity**.  \n",
        "- You want **better lemmatization and pre-trained word vectors**.  \n",
        "\n",
        "---\n",
        "\n",
        "## **🔍 Quick Comparison Table**\n",
        "\n",
        "| Feature                | **NLTK** 🟢 | **spaCy** 🔵 |\n",
        "|------------------------|------------|--------------|\n",
        "| **Ease of Use**        | More manual setup  | Easier, built-in pipelines |\n",
        "| **Speed**             | Slower (pure Python)  | Faster (Cython-based) |\n",
        "| **Lemmatization**      | Uses WordNet (requires POS tagging)  | More accurate, built-in |\n",
        "| **Tokenization**       | Rule-based, can be customized  | More efficient, pre-trained |\n",
        "| **Stopword Removal**   | Built-in list, manual removal  | Built-in but disabled by default |\n",
        "| **Part-of-Speech (POS) Tagging** | Available, requires additional processing  | Faster and more accurate |\n",
        "| **Named Entity Recognition (NER)** | Limited, needs training  | Pre-trained and efficient |\n",
        "| **Dependency Parsing** | Limited support  | Built-in and optimized |\n",
        "| **Best For**           | Research, rule-based NLP, small projects  | Production, large-scale NLP tasks |\n",
        "\n",
        "---\n",
        "\n",
        " *** When to Use Both Together? ***\n",
        "\n",
        "🔗 **Use NLTK for preprocessing (e.g., stopword removal, stemming) and spaCy for NLP tasks like NER, dependency parsing, and text similarity.**  \n",
        "\n",
        "\n",
        "\n",
        "## **🔥 Final Verdict**\n",
        "- **Use NLTK** if you need flexibility and **custom NLP pipelines** for research.  \n",
        "- **Use spaCy** if you need **fast, pre-trained NLP models for production**.  \n",
        "- **Use both** when you need **NLTK for preprocessing** and **spaCy for advanced NLP tasks** like NER and dependency parsing.  "
      ],
      "metadata": {
        "id": "C0nSxpfrq-Z2"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GpGr-I7arig7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}